{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1YisNvZxrkSxzUUEgw9EAJJ1kgwp3UQLx",
      "authorship_tag": "ABX9TyMCw/eDa67ltLwNycuB5Y/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioGzSl/word2vec/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "RWgEs7MgHJRo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "dataset_wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\n",
        "\n",
        "N = 1000\n",
        "text = []\n",
        "for i in range(N):\n",
        "    article = dataset_wikipedia[i]['text']\n",
        "    text.extend(preprocess(article))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def create_vocabulary(text):\n",
        "    word_counts = Counter(text)\n",
        "    vocabulary = {word: idx for idx, word in enumerate(word_counts)}\n",
        "    return vocabulary, word_counts"
      ],
      "metadata": {
        "id": "DnCcOXxrH7F0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, text, window_size=2):\n",
        "        self.text = text\n",
        "        self.window_size = window_size\n",
        "        self.vocabulary, self.word_counts = create_vocabulary(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      target_word = self.text[idx]\n",
        "      target_idx = self.vocabulary[target_word]\n",
        "\n",
        "      start = idx - self.window_size\n",
        "      end = idx + self.window_size + 1\n",
        "\n",
        "      context_indices = []\n",
        "      for i in range(start, end):\n",
        "          if i != idx and 0 <= i < len(self.text):\n",
        "              context_indices.append(self.vocabulary[self.text[i]])\n",
        "          elif i != idx:\n",
        "              context_indices.append(0)\n",
        "\n",
        "      context_tensor = torch.tensor(context_indices, dtype=torch.long)\n",
        "      target_tensor = torch.tensor(target_idx, dtype=torch.long)\n",
        "\n",
        "      return context_tensor, target_tensor\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target_idxs):\n",
        "        target_embeds = self.embeddings(target_idxs)\n",
        "        out = self.output_layer(target_embeds)\n",
        "        log_probs = torch.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "GOWX9eJCIPBg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SkipGramDataset(text)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vocab_size = len(dataset.vocabulary)\n",
        "embedding_dim = 128\n",
        "\n",
        "model = SkipGramModel(vocab_size, embedding_dim).to(device)"
      ],
      "metadata": {
        "id": "EQm9Xb9Oikkm"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for context, target in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        target, context = target.to(device), context.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        log_probs = model(target)\n",
        "\n",
        "        log_probs = log_probs.unsqueeze(1).repeat(1, context.size(1), 1)\n",
        "        log_probs = log_probs.view(-1, log_probs.size(-1))\n",
        "\n",
        "        context = context.view(-1)\n",
        "        loss = loss_function(log_probs, context)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}, Total loss: {total_loss}\")\n",
        "    if(epoch%10 == 0):\n",
        "      torch.save(model.embeddings.state_dict(), f\"/content/drive/MyDrive/word2vec/word_embeddings_chk_'{epoch/10}'.pth\")\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.embeddings.state_dict(), '/content/drive/MyDrive/word2vec/word_embeddings.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "1yo-96gMJW4E",
        "outputId": "803cd0c1-fb4e-4302-f017-975f4f76a8ae"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 27693/27693 [21:34<00:00, 21.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Total loss: 315992.89405441284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 27693/27693 [21:35<00:00, 21.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Total loss: 289394.6640806198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3:  50%|████▉     | 13812/27693 [10:46<10:49, 21.37it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-70cddb688a5d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}, Total loss: {total_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inverse_vocabulary(vocabulary):\n",
        "    inverse_vocabulary = {idx: word for word, idx in vocabulary.items()}\n",
        "    return inverse_vocabulary\n",
        "\n",
        "vocabulary, inverse_vocabulary = dataset.vocabulary, create_inverse_vocabulary(dataset.vocabulary)\n",
        "\n",
        "def evaluate_word(model, word, vocabulary, inverse_vocabulary, device='cpu'):\n",
        "    model.to(device).eval()\n",
        "\n",
        "    if word in vocabulary:\n",
        "        word_idx = torch.tensor([vocabulary[word]], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            log_probs = model(word_idx)\n",
        "\n",
        "        probs = torch.exp(log_probs).squeeze(0)\n",
        "\n",
        "        topk_values, topk_indices = probs.topk(100, largest=True, sorted=True)\n",
        "\n",
        "        top_words = [inverse_vocabulary[idx.item()] for idx in topk_indices]\n",
        "\n",
        "        return top_words, topk_values\n",
        "    else:\n",
        "        return f\"La palabra '{word}' no se encontró en el vocabulario.\", None\n",
        "\n",
        "def get_embedding(model, word, device='cpu'):\n",
        "    model.to(device).eval()\n",
        "\n",
        "    if word in vocabulary:\n",
        "      word_idx = torch.tensor([vocabulary[word]], dtype=torch.long).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        word_embedding = model.embeddings(word_idx)\n",
        "    return word_embedding\n",
        "\n",
        "def get_words(word_embedding, vocabulary, inverse_vocabulary, device='cpu'):\n",
        "    word_embedding = word_embedding.to(device)\n",
        "\n",
        "    all_embeddings = torch.stack([model.embeddings(torch.tensor([idx])) for idx in range(len(vocabulary))]).squeeze(1)\n",
        "\n",
        "    similarities = torch.nn.functional.cosine_similarity(word_embedding, all_embeddings)\n",
        "\n",
        "    topk_values, topk_indices = similarities.topk(120, largest=True, sorted=True)\n",
        "\n",
        "    closest_words = [inverse_vocabulary[idx.item()] for idx in topk_indices]\n",
        "\n",
        "    return closest_words, topk_values\n"
      ],
      "metadata": {
        "id": "93EUX356KPoo"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = get_embedding(model, \"morning\")"
      ],
      "metadata": {
        "id": "sRPHlAFAyu6C"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_words(embedding, vocabulary, inverse_vocabulary, device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icSxgh9Vy-h4",
        "outputId": "259360e4-7f52-47d4-d218-fcf317021f0b"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['morning',\n",
              "  'assam,',\n",
              "  'schatz.',\n",
              "  '\"mentality\"',\n",
              "  'chatti.',\n",
              "  'andranik',\n",
              "  'nobilis,',\n",
              "  'over-zealous',\n",
              "  'one-off',\n",
              "  'irrespective',\n",
              "  'loses',\n",
              "  'tegmen',\n",
              "  'djurdjevic',\n",
              "  'somatic',\n",
              "  'saško)',\n",
              "  'army.\"',\n",
              "  'apple-1',\n",
              "  '1851',\n",
              "  '1–83\",',\n",
              "  'howerd,',\n",
              "  '(leu,',\n",
              "  'gulag,',\n",
              "  'sarayı',\n",
              "  'quietly.',\n",
              "  'fältskog,',\n",
              "  'heythrop',\n",
              "  'poulin,',\n",
              "  'relinquishing',\n",
              "  'mouseion,',\n",
              "  'conceptacles',\n",
              "  'refuge.',\n",
              "  'remittance',\n",
              "  'chapter)',\n",
              "  'fireproof',\n",
              "  'hilal',\n",
              "  'non-combatants',\n",
              "  'sommerville.',\n",
              "  'analog/hybrid',\n",
              "  'catulus.',\n",
              "  'scandalized',\n",
              "  'mayock',\n",
              "  'yellow-orange',\n",
              "  'tone),',\n",
              "  'suitors.',\n",
              "  'lombardia',\n",
              "  'mersenne',\n",
              "  'given,',\n",
              "  \"mouth'.\",\n",
              "  'eliyahu,',\n",
              "  'whatever',\n",
              "  'balasko,',\n",
              "  '597.',\n",
              "  '5,610',\n",
              "  'estate\".',\n",
              "  'p2000',\n",
              "  'philosophers\".',\n",
              "  'achillia,',\n",
              "  'taronites,',\n",
              "  'lyon).',\n",
              "  'coucy',\n",
              "  \"d'encouragement\",\n",
              "  'hardening,',\n",
              "  'heat,',\n",
              "  'expo,',\n",
              "  'domagoj',\n",
              "  'relationship.\"',\n",
              "  'monahan,',\n",
              "  'poisoning',\n",
              "  'quaraouiyine',\n",
              "  'juárez',\n",
              "  'ap.',\n",
              "  '(cev)\"',\n",
              "  'egyptian-american',\n",
              "  'solidity',\n",
              "  'natwarlal',\n",
              "  '\"alexikakos\"',\n",
              "  'opulence\"—for',\n",
              "  'controller,',\n",
              "  'sub-disciplines',\n",
              "  'loneliness',\n",
              "  'philosophie,',\n",
              "  'thrombus',\n",
              "  \"hifolks'\",\n",
              "  'vlachs,',\n",
              "  'mnemonic',\n",
              "  'postmodernist',\n",
              "  'intertestamental',\n",
              "  'extensionality',\n",
              "  'pre-parthenon,',\n",
              "  'deviation:',\n",
              "  'illuminati.',\n",
              "  '2:1–15),',\n",
              "  'closed-captioned',\n",
              "  'amide.',\n",
              "  'dio).',\n",
              "  'colours:',\n",
              "  'employs,',\n",
              "  \"keats'\",\n",
              "  '(windbeutel),',\n",
              "  'kudryashov,',\n",
              "  'pryce,',\n",
              "  'doi:',\n",
              "  'dependent,',\n",
              "  'palaw',\n",
              "  'rome),',\n",
              "  'potestas',\n",
              "  '2700,',\n",
              "  'scepticism',\n",
              "  'possibility',\n",
              "  '29402',\n",
              "  '\"something',\n",
              "  '\"ultra-violence\"',\n",
              "  '(tugboat),',\n",
              "  'pronunciation,',\n",
              "  'epos',\n",
              "  'benjamín',\n",
              "  'lyrical',\n",
              "  'web-hosted',\n",
              "  'respectively.',\n",
              "  'computers'],\n",
              " tensor([1.0000, 0.3697, 0.3558, 0.3544, 0.3487, 0.3465, 0.3441, 0.3437, 0.3416,\n",
              "         0.3415, 0.3397, 0.3384, 0.3382, 0.3380, 0.3362, 0.3333, 0.3328, 0.3316,\n",
              "         0.3309, 0.3290, 0.3272, 0.3259, 0.3255, 0.3241, 0.3240, 0.3238, 0.3227,\n",
              "         0.3211, 0.3206, 0.3196, 0.3193, 0.3180, 0.3169, 0.3168, 0.3168, 0.3158,\n",
              "         0.3155, 0.3153, 0.3152, 0.3151, 0.3144, 0.3141, 0.3125, 0.3119, 0.3119,\n",
              "         0.3112, 0.3110, 0.3110, 0.3098, 0.3098, 0.3092, 0.3089, 0.3075, 0.3072,\n",
              "         0.3072, 0.3064, 0.3057, 0.3057, 0.3053, 0.3049, 0.3047, 0.3038, 0.3037,\n",
              "         0.3036, 0.3034, 0.3030, 0.3029, 0.3029, 0.3029, 0.3028, 0.3025, 0.3024,\n",
              "         0.3024, 0.3017, 0.3013, 0.3012, 0.3001, 0.3000, 0.2999, 0.2998, 0.2995,\n",
              "         0.2987, 0.2986, 0.2984, 0.2983, 0.2975, 0.2975, 0.2975, 0.2973, 0.2972,\n",
              "         0.2972, 0.2972, 0.2971, 0.2970, 0.2969, 0.2969, 0.2966, 0.2961, 0.2960,\n",
              "         0.2960, 0.2959, 0.2955, 0.2954, 0.2952, 0.2947, 0.2946, 0.2942, 0.2942,\n",
              "         0.2939, 0.2939, 0.2938, 0.2936, 0.2933, 0.2932, 0.2932, 0.2932, 0.2931,\n",
              "         0.2931, 0.2930, 0.2926], grad_fn=<TopkBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_test = \"woman\"\n",
        "top_words, top_values = evaluate_word(model, word_to_test, vocabulary, inverse_vocabulary, device='cpu')\n",
        "print(f\"Las palabras más probables para '{word_to_test}' son:\")\n",
        "for word, value in zip(top_words, top_values):\n",
        "    print(f\"{word}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "Ty8MhRM7fmF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ed53dc-c279-482a-8760-151c370ac885"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las palabras más probables para 'woman' son:\n",
            "the: 0.0792\n",
            "a: 0.0318\n",
            "in: 0.0316\n",
            "of: 0.0311\n",
            "and: 0.0275\n",
            "to: 0.0174\n",
            "as: 0.0112\n",
            "is: 0.0028\n",
            "–: 0.0012\n",
            "that: 0.0009\n",
            "by: 0.0008\n",
            "was: 0.0007\n",
            "for: 0.0006\n",
            "with: 0.0005\n",
            "on: 0.0003\n",
            "from: 0.0003\n",
            "he: 0.0001\n",
            "an: 0.0001\n",
            "it: 0.0001\n",
            "his: 0.0001\n",
            "at: 0.0001\n",
            "has: 0.0001\n",
            "had: 0.0001\n",
            "have: 0.0001\n",
            "not: 0.0001\n",
            "or: 0.0000\n",
            "this: 0.0000\n",
            "also: 0.0000\n",
            "which: 0.0000\n",
            "are: 0.0000\n",
            "cinema.\": 0.0000\n",
            "but: 0.0000\n",
            "kraivichien,: 0.0000\n",
            "husayn: 0.0000\n",
            "bee's: 0.0000\n",
            "arnbitter,: 0.0000\n",
            "spyrka: 0.0000\n",
            "be: 0.0000\n",
            "one: 0.0000\n",
            "(b.: 0.0000\n",
            "cristo: 0.0000\n",
            "insisted: 0.0000\n",
            "tunnels: 0.0000\n",
            "expression.\": 0.0000\n",
            "dormancy.: 0.0000\n",
            "mirabili: 0.0000\n",
            "vesuviana,: 0.0000\n",
            "rubber,: 0.0000\n",
            "(bell's: 0.0000\n",
            "mechanic.: 0.0000\n",
            "¥2.15: 0.0000\n",
            "viscosity.: 0.0000\n",
            "1239): 0.0000\n",
            "(d.: 0.0000\n",
            "theaters: 0.0000\n",
            "mavroneri: 0.0000\n",
            "chiklis,: 0.0000\n",
            "resolved?\".: 0.0000\n",
            "afford.: 0.0000\n",
            "physik: 0.0000\n",
            "antigonia.: 0.0000\n",
            "oistrophe,: 0.0000\n",
            "thoughts,: 0.0000\n",
            "capo: 0.0000\n",
            "iiis: 0.0000\n",
            "realtoo: 0.0000\n",
            "conduction.: 0.0000\n",
            "adamović,: 0.0000\n",
            "clam,: 0.0000\n",
            "rapist.: 0.0000\n",
            "boundaries\": 0.0000\n",
            "(asc): 0.0000\n",
            "priori): 0.0000\n",
            "(1983): 0.0000\n",
            "harim:: 0.0000\n",
            "unresolved:: 0.0000\n",
            "iaunched: 0.0000\n",
            "fatal): 0.0000\n",
            "fourth: 0.0000\n",
            "barrett: 0.0000\n",
            "verse:: 0.0000\n",
            "wajda: 0.0000\n",
            "(embrun): 0.0000\n",
            "nitride: 0.0000\n",
            "who: 0.0000\n",
            "snout): 0.0000\n",
            "tetracyclic: 0.0000\n",
            "crichton-stuart,: 0.0000\n",
            "discloses: 0.0000\n",
            "fundo: 0.0000\n",
            "spacecraft—two: 0.0000\n",
            "between: 0.0000\n",
            "x3: 0.0000\n",
            "miracoli: 0.0000\n",
            "turmoil.: 0.0000\n",
            "\"irrepressible: 0.0000\n",
            "hatif,: 0.0000\n",
            "colspan=5|: 0.0000\n",
            "sonorous: 0.0000\n",
            "influx: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VU8A630fw0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}